{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective : To try hand at feature engineering \n",
    "- learn the importance \n",
    "- some techniques\n",
    "\n",
    "\n",
    "Dataset : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(r'./Data/feature_engineering/train.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159571 entries, 0000997932d777bf to fff46fc426af1f9a\n",
      "Data columns (total 7 columns):\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 9.7+ MB\n",
      "None\n",
      "               toxic   severe_toxic        obscene         threat  \\\n",
      "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
      "mean        0.095844       0.009996       0.052948       0.002996   \n",
      "std         0.294379       0.099477       0.223931       0.054650   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "              insult  identity_hate  \n",
      "count  159571.000000  159571.000000  \n",
      "mean        0.049364       0.008805  \n",
      "std         0.216627       0.093420  \n",
      "min         0.000000       0.000000  \n",
      "25%         0.000000       0.000000  \n",
      "50%         0.000000       0.000000  \n",
      "75%         0.000000       0.000000  \n",
      "max         1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())\n",
    "print(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking the null values\n",
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ffe987279560d7ff</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffea4adeee384e90</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffee36eab5c267c9</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff125370e4aaaf3</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff46fc426af1f9a</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "ffe987279560d7ff  \":::::And for the second time of asking, when ...      0   \n",
       "ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "fff125370e4aaaf3  And it looks like it was actually you who put ...      0   \n",
       "fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "ffe987279560d7ff             0        0       0       0              0  \n",
       "ffea4adeee384e90             0        0       0       0              0  \n",
       "ffee36eab5c267c9             0        0       0       0              0  \n",
       "fff125370e4aaaf3             0        0       0       0              0  \n",
       "fff46fc426af1f9a             0        0       0       0              0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "ffe987279560d7ff    \":::::And for the second time of asking, when ...\n",
       "ffea4adeee384e90    You should be ashamed of yourself \\n\\nThat is ...\n",
       "ffee36eab5c267c9    Spitzer \\n\\nUmm, theres no actual article for ...\n",
       "fff125370e4aaaf3    And it looks like it was actually you who put ...\n",
       "fff46fc426af1f9a    \"\\nAnd ... I really don't think you understand...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['comment_text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            15294\n",
       "severe_toxic      1595\n",
       "obscene           8449\n",
       "threat             478\n",
       "insult            7877\n",
       "identity_hate     1405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[:,['toxic','severe_toxic','obscene','threat','insult','identity_hate']].apply(sum, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING UP THE TEXT\n",
    "#Function to clean up the text\n",
    "def standardizetext(df, textfield):\n",
    "    df[textfield] = df[textfield].str.replace(r\"http\\S+\", \"\")\n",
    "    df[textfield] = df[textfield].str.replace(r\"http\", \"\")\n",
    "    df[textfield] = df[textfield].str.replace(r\"@\\S+\", \"\")\n",
    "    df[textfield] = df[textfield].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[textfield] = df[textfield].str.replace(r\"@\", \"at\")\n",
    "    df[textfield] = df[textfield].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>total_length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_smilies</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>hey man, i'm really not trying to edit war  it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>82</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>you, sir, are my hero  any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  explanation\\nwhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  d'aww! he matches this background colour i'm s...      0   \n",
       "000113f07ec002fd  hey man, i'm really not trying to edit war  it...      0   \n",
       "0001b41b1c6bb37e  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  you, sir, are my hero  any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0000997932d777bf             0        0       0       0              0   \n",
       "000103f0d9cfb60f             0        0       0       0              0   \n",
       "000113f07ec002fd             0        0       0       0              0   \n",
       "0001b41b1c6bb37e             0        0       0       0              0   \n",
       "0001d958c54c6e35             0        0       0       0              0   \n",
       "\n",
       "                  total_length  capitals  num_exclamation_marks  \\\n",
       "id                                                                \n",
       "0000997932d777bf           264         0                      0   \n",
       "000103f0d9cfb60f           112         0                      1   \n",
       "000113f07ec002fd           233         0                      0   \n",
       "0001b41b1c6bb37e           622         0                      0   \n",
       "0001d958c54c6e35            67         0                      0   \n",
       "\n",
       "                  num_question_marks  num_punctuation  num_symbols  num_words  \\\n",
       "id                                                                              \n",
       "0000997932d777bf                   1                1            0         47   \n",
       "000103f0d9cfb60f                   0                2            0         18   \n",
       "000113f07ec002fd                   0                1            0         42   \n",
       "0001b41b1c6bb37e                   0                2            0        114   \n",
       "0001d958c54c6e35                   1                2            0         13   \n",
       "\n",
       "                  num_unique_words  words_vs_unique  num_smilies  \n",
       "id                                                                \n",
       "0000997932d777bf                45         0.957447            0  \n",
       "000103f0d9cfb60f                18         1.000000            0  \n",
       "000113f07ec002fd                39         0.928571            0  \n",
       "0001b41b1c6bb37e                82         0.719298            0  \n",
       "0001d958c54c6e35                13         1.000000            0  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = standardizetext(df_train, \"comment_text\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['comment_text']\n",
    "Y = df_train.drop(['comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.8, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing using Term frequency and inter document frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(decode_error='ignore',  min_df=0.009,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.drop(['vectorized'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf_vectorizer.fit_transform(X_train)\n",
    "X_test = tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn to use oneVsRest multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(['comment_text'],axis=1)\n",
    "# X_test =X_test.drop(['comment_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.dropna(axis=1)\n",
    "# X_test.isnull().sum()\n",
    "# X_test = X_test.dropna(axis=1)#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OneVsRestClassifier(LinearSVC())\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9073476421745261"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score on test dataset\n",
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079009212257944"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score on train dataset\n",
    "model.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88d640ab9223398d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9f9b412e0a99e6dc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323848f9f866c47e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90bd2ed36352d163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe77e59403ac395e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17e22e51f52ae4a4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d80e1ab696b108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9329101468aa1b93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4c4cae190527b2a1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e9c6a8772267b3c</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "id                                                                           \n",
       "88d640ab9223398d      0             0        0       0       0              0\n",
       "9f9b412e0a99e6dc      0             0        0       0       0              0\n",
       "323848f9f866c47e      0             0        0       0       0              0\n",
       "90bd2ed36352d163      0             0        0       0       0              0\n",
       "fe77e59403ac395e      0             0        0       0       0              0\n",
       "17e22e51f52ae4a4      0             0        0       0       0              0\n",
       "b1d80e1ab696b108      0             0        0       0       0              0\n",
       "9329101468aa1b93      0             0        0       0       0              0\n",
       "4c4cae190527b2a1      0             0        0       0       0              0\n",
       "8e9c6a8772267b3c      1             0        0       0       0              0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_v_one = OneVsOneClassifier(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_one_v_one.fit(X_train,Y_train)\n",
    "# model_one_v_one.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using Multiple Output regressor -\n",
    "\n",
    "Multiple out regressor fits multiple regressor - this stratergy fits one regressor per target. Since each traget is represented by exactly one regressor  it is possible to drive strength out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = MultiOutputClassifier(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "           n_jobs=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multi_model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9054989816700612"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9103214890016921"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the score of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, svm, tree, ensemble\n",
    "\n",
    "models = [\n",
    "#     linear_model.LogisticRegressionCV(cv =5),\n",
    "#     linear_model.RidgeClassifier(),\n",
    "    OneVsRestClassifier(linear_model.LogisticRegression(solver= 'newton-cg')),\n",
    "    OneVsRestClassifier(linear_model.LogisticRegression(solver= 'sag')),\n",
    "\n",
    "    OneVsRestClassifier(linear_model.RidgeClassifier(solver= 'sag')),\n",
    "    OneVsRestClassifier(linear_model.SGDClassifier()),\n",
    "#     linear_model.RidgeClassifier(solver= 'newton-cg'),\n",
    "\n",
    "]\n",
    "#     linear_model.RidgeClassifierCV(),\n",
    "#     linear_model.Lasso(),\n",
    "#     linear_model.ElasticNet(),\n",
    "#     linear_model.BayesianRidge(),\n",
    "# #     linear_model.RANSACRegressor(),\n",
    "#     svm.LinearSVR(),\n",
    "#     svm.SVR(),\n",
    "#     tree.DecisionTreeRegressor(),\n",
    "#     tree.ExtraTreeRegressor(),\n",
    "#     ensemble.RandomForestRegressor(),\n",
    "#     ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# ]\n",
    "\n",
    "# Ridge classifier does not support multiclass classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "OneVsRestClassifier\n",
      "Root mean square error train 0.16238547930398348\n",
      "Accuracy score for traing set 0.9075797455662092\n",
      "Root mean square error test 0.16298882137514256\n",
      "Accuracy score for test set 0.9070343098856337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "OneVsRestClassifier\n",
      "Root mean square error train 0.19082181218989785\n",
      "Accuracy score for traing set 0.897952309331328\n",
      "Root mean square error test 0.19193080202018906\n",
      "Accuracy score for test set 0.8965690114366286\n",
      "------------------------------\n",
      "OneVsRestClassifier\n",
      "Root mean square error train 0.19093467088807392\n",
      "Accuracy score for traing set 0.8985711599924798\n",
      "Root mean square error test 0.19218911169885664\n",
      "Accuracy score for test set 0.8971330095566348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\amitkumar_kataria\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "OneVsRestClassifier\n",
      "Root mean square error train 0.19833022753577959\n",
      "Accuracy score for traing set 0.8841104217584759\n",
      "Root mean square error test 0.19978316095225118\n",
      "Accuracy score for test set 0.8820303932320226\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train,Y_train)\n",
    "    print('-'*30)\n",
    "    print(model.__class__.__name__)\n",
    "#     print(model.classes_)\n",
    "#     print(model.estimators_)\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(Y_train, train_pred))\n",
    "    print(\"Root mean square error train {}\".format(train_rmse))    \n",
    "    print(\"Accuracy score for traing set {}\".format(model.score(X_train,Y_train)))\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "    print(\"Root mean square error test {}\".format(rmse))\n",
    "    print(\"Accuracy score for test set {}\".format(model.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Getting into Feature Engineering\n",
    "As we see above we have trained 4 different models but our accuracy is not going above 90% to enhance the accuracy we are going to create some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 455)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate', 'total_length', 'capitals', 'num_exclamation_marks',\n",
      "       'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words',\n",
      "       'num_unique_words', 'words_vs_unique', 'num_smilies'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['total_length'] = df_train['comment_text'].apply(len)\n",
    "df_train['capitals'] = df_train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "# # df_train['caps_vs_length'] = df_train.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "# #                                 axis=1)\n",
    "df_train['num_exclamation_marks'] = df_train['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "df_train['num_question_marks'] = df_train['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "df_train['num_punctuation'] = df_train['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "df_train['num_symbols'] = df_train['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "df_train['num_words'] = df_train['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "df_train['num_unique_words'] = df_train['comment_text'].apply(\n",
    "    lambda comment: len(set(w for w in comment.split())))\n",
    "df_train['words_vs_unique'] = df_train['num_unique_words'] / df_train['num_words']\n",
    "df_train['num_smilies'] = df_train['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['comment_text',  'total_length', 'capitals', 'num_exclamation_marks',\n",
    "       'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words',\n",
    "       'num_unique_words', 'words_vs_unique', 'num_smilies']]\n",
    "Y = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.8, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "coments_X_train = tf_vectorizer.fit_transform(X_train['comment_text'])\n",
    "comments_X_test = tf_vectorizer.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_featuers_spars_X_train = csr_matrix(X_train[['total_length', 'capitals', 'num_exclamation_marks',\n",
    "       'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words',\n",
    "       'num_unique_words', 'words_vs_unique', 'num_smilies']])\n",
    "\n",
    "other_featuers_spars_X_test = csr_matrix(X_test[['total_length', 'capitals', 'num_exclamation_marks',\n",
    "       'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words',\n",
    "       'num_unique_words', 'words_vs_unique', 'num_smilies']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 455)\n",
      "(127656, 10)\n",
      "(127656, 465)\n"
     ]
    }
   ],
   "source": [
    "# combined = hstack([other_spares,X_train])\n",
    "# print(other_spares.shape)\n",
    "print(coments_X_train.shape)\n",
    "print(other_featuers_spars_X_train.shape)\n",
    "print(X_train.shape)\n",
    "# print(coments_X_train.isnull().sum())\n",
    "# print(other_featuers_spars_X_train.isnull().sum())\n",
    "# print(X_train.isnull().sum())\n",
    "# \n",
    "# print(other_spares.__class__)\n",
    "# print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_train.data).any()\n",
    "# other_featuers_spars_X_test.data = np.nan_to_num(other_featuers_spars_X_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([coments_X_train, other_featuers_spars_X_train])\n",
    "X_test = hstack([comments_X_test, other_featuers_spars_X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test only for - UnderStanding TfidfVectorizer\n",
    "NOt the part of acutal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_train = df_train['comment_text']\n",
    "\n",
    "x_df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_vectorizer = TfidfVectorizer(decode_error='ignore',  min_df=0.009,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = tf_vectorizer.fit_transform(x_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 454)\n",
      "['10', '100', '11', '12', '14', '15', '20', '2005', '2006', '2007', '2008', '24', 'able', 'according', 'account', 'actual', 'actually', 'add', 'added', 'adding', 'address', 'admin', 'administrator', 'admins', 'ago', 'agree', 'allowed', 'american', 'answer', 'anti', 'appear', 'appears', 'appreciate', 'appropriate', 'aren', 'argument', 'article', 'articles', 'ask', 'asked', 'attack', 'attacks', 'attention', 'automatically', 'aware', 'away', 'bad', 'based', 'believe', 'best', 'better', 'big', 'bit', 'block', 'blocked', 'book', 'books', 'called', 'came', 'care', 'case', 'category', 'certain', 'certainly', 'change', 'changed', 'changes', 'check', 'cheers', 'cited', 'claim', 'claims', 'clear', 'clearly', 'come', 'comment', 'comments', 'common', 'community', 'completely', 'consensus', 'consider', 'considered', 'contact', 'content', 'continue', 'contribs', 'contributing', 'contributions', 'copyright', 'correct', 'country', 'course', 'create', 'created', 'criteria', 'current', 'currently', 'date', 'day', 'days', 'decide', 'delete', 'deleted', 'deleting', 'deletion', 'description', 'did', 'didn', 'different', 'disagree', 'discuss', 'discussion', 'dispute', 'does', 'doesn', 'doing', 'don', 'dont', 'edit', 'edited', 'editing', 'editor', 'editors', 'edits', 'encyclopedia', 'end', 'english', 'enjoy', 'entire', 'entry', 'especially', 'evidence', 'exactly', 'example', 'explain', 'fact', 'facts', 'fair', 'faith', 'false', 'far', 'feel', 'fine', 'follow', 'following', 'form', 'free', 'fuck', 'fucking', 'future', 'general', 'generally', 'getting', 'given', 'going', 'good', 'got', 'great', 'group', 'guess', 'guidelines', 'guy', 'happy', 'hard', 'haven', 'having', 'hello', 'help', 'hey', 'hi', 'high', 'history', 'hope', 'idea', 'image', 'images', 'important', 'include', 'included', 'including', 'info', 'information', 'instead', 'interested', 'interesting', 'involved', 'ip', 'isn', 'issue', 'issues', 'just', 'kind', 'know', 'knowledge', 'known', 'language', 'later', 'lead', 'learn', 'leave', 'left', 'let', 'life', 'like', 'likely', 'line', 'link', 'links', 'list', 'listed', 'little', 'live', 'll', 'long', 'look', 'looking', 'looks', 'lot', 'love', 'main', 'major', 'make', 'makes', 'making', 'man', 'material', 'matter', 'maybe', 'mean', 'means', 'media', 'mention', 'mentioned', 'message', 'mind', 'moved', 'names', 'need', 'needed', 'needs', 'neutral', 'new', 'news', 'nice', 'non', 'notability', 'notable', 'note', 'notice', 'noticed', 'npov', 'number', 'obvious', 'obviously', 'official', 'oh', 'ok', 'old', 'open', 'opinion', 'order', 'original', 'page', 'pages', 'paragraph', 'particular', 'party', 'past', 'people', 'person', 'personal', 'picture', 'place', 'placed', 'point', 'points', 'policies', 'policy', 'political', 'position', 'possible', 'post', 'posted', 'pov', 'power', 'present', 'pretty', 'probably', 'problem', 'problems', 'process', 'project', 'provide', 'provided', 'public', 'published', 'question', 'questions', 'quite', 'quote', 'read', 'reading', 'real', 'really', 'reason', 'reasons', 'recent', 'recently', 'redirect', 'reference', 'references', 'regarding', 'regards', 'related', 'relevant', 'reliable', 'remember', 'remove', 'removed', 'removing', 'reply', 'report', 'request', 'research', 'response', 'revert', 'reverted', 'reverting', 'review', 'right', 'rule', 'rules', 'said', 'sandbox', 'saw', 'say', 'saying', 'says', 'school', 'search', 'second', 'section', 'seen', 'self', 'sense', 'sentence', 'set', 'shit', 'short', 'shows', 'sign', 'similar', 'simple', 'simply', 'single', 'site', 'small', 'soon', 'sorry', 'sort', 'source', 'sources', 'specific', 'speedy', 'start', 'started', 'state', 'stated', 'statement', 'states', 'status', 'stay', 'stop', 'stuff', 'stupid', 'style', 'subject', 'suggest', 'summary', 'support', 'sure', 'tag', 'taken', 'taking', 'talk', 'talking', 'tell', 'template', 'term', 'terms', 'text', 'thank', 'thanks', 'thing', 'things', 'think', 'thought', 'time', 'times', 'title', 'today', 'took', 'topic', 'tried', 'true', 'truth', 'try', 'trying', 'type', 'understand', 'unless', 'use', 'used', 'useful', 'user', 'users', 'using', 'utc', 'vandalism', 'vandalize', 'various', 've', 'version', 'view', 'want', 'wanted', 'war', 'warning', 'wasn', 'way', 'web', 'website', 'week', 'welcome', 'wiki', 'wikipedia', 'wish', 'won', 'word', 'words', 'work', 'working', 'world', 'wouldn', 'wp', 'write', 'writing', 'written', 'wrong', 'wrote', 'year', 'years', 'yes']\n",
      "454\n"
     ]
    }
   ],
   "source": [
    "print(trans.shape)\n",
    "print(tf_vectorizer.get_feature_names())\n",
    "print(len(tf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyzer': 'word', 'binary': False, 'decode_error': 'ignore', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 0.009, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': 'english', 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'call', 'became', 'few', 'to', 'are', 'although', 'anywhere', 'above', 'even', 'for', 'system', 'herself', 'thereupon', 'whose', 'yourselves', 'every', 'whoever', 'how', 'or', 'off', 'whereby', 'interest', 'keep', 'out', 'bill', 'de', 'name', 'should', 'sincere', 'fire', 'nobody', 'same', 'nor', 'her', 'cry', 'un', 'not', 'can', 'eight', 'next', 'on', 'serious', 'several', 'though', 'whole', 'yourself', 'get', 'others', 'hundred', 'anyone', 'now', 'meanwhile', 're', 'empty', 'give', 'between', 'and', 'down', 'everyone', 'somehow', 'through', 'also', 'than', 'am', 'very', 'ever', 'put', 'he', 'seeming', 'behind', 'last', 'latterly', 'someone', 'made', 'be', 'third', 'thru', 'within', 'only', 'whatever', 'cant', 'again', 'anything', 'still', 'but', 'into', 'is', 'where', 'four', 'formerly', 'however', 'least', 'side', 'ten', 'as', 'enough', 'six', 'top', 'after', 'while', 'will', 'against', 'elsewhere', 'thus', 'move', 'amongst', 'across', 'somewhere', 'hers', 'one', 'per', 'whither', 'almost', 'may', 'below', 'your', 'many', 'seem', 'own', 'namely', 'onto', 'some', 'here', 'find', 'before', 'former', 'therefore', 'us', 'nothing', 'fill', 'each', 'part', 'towards', 'have', 'indeed', 'whence', 'cannot', 'less', 'nowhere', 'them', 'becomes', 'because', 'his', 'three', 'twelve', 'thereby', 'were', 'whereupon', 'what', 'full', 'ltd', 'would', 'more', 'something', 'among', 'must', 'thereafter', 'nine', 'take', 'over', 'himself', 'amount', 'together', 'when', 'two', 'therein', 'themselves', 'those', 'their', 'via', 'do', 'etc', 'rather', 'that', 'whether', 'alone', 'con', 'everything', 'anyway', 'sixty', 'they', 'from', 'thick', 'me', 'upon', 'we', 'without', 'been', 'hence', 'except', 'everywhere', 'afterwards', 'otherwise', 'the', 'too', 'well', 'him', 'itself', 'an', 'toward', 'if', 'moreover', 'yours', 'already', 'co', 'mill', 'was', 'detail', 'ourselves', 'fifty', 'bottom', 'which', 'these', 'whereafter', 'a', 'wherever', 'hereafter', 'thin', 'hasnt', 'she', 'sometime', 'any', 'seemed', 'whom', 'none', 'becoming', 'other', 'describe', 'then', 'amoungst', 'mine', 'thence', 'forty', 'either', 'else', 'had', 'hereupon', 'inc', 'perhaps', 'yet', 'this', 'done', 'our', 'twenty', 'until', 'beside', 'latter', 'being', 'could', 'due', 'eleven', 'become', 'found', 'mostly', 'my', 'show', 'such', 'neither', 'front', 'couldnt', 'always', 'eg', 'you', 'since', 'along', 'by', 'whenever', 'most', 'so', 'throughout', 'i', 'with', 'seems', 'beyond', 'further', 'of', 'sometimes', 'up', 'under', 'often', 'nevertheless', 'about', 'noone', 'ours', 'besides', 'no', 'wherein', 'during', 'five', 'might', 'who', 'all', 'at', 'never', 'please', 'herein', 'its', 'see', 'around', 'there', 'another', 'whereas', 'it', 'in', 'has', 'ie', 'anyhow', 'hereby', 'why', 'beforehand', 'much', 'both', 'fifteen', 'once', 'first', 'myself', 'go', 'back'})\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary -- Take Aways\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References :\n",
    "https://www.kaggle.com/eikedehling/feature-engineering\n",
    "\n",
    "[Multi Class Classification](# https://www.youtube.com/watch?v=y0wQK3mnrNY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
